{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Object Segmentation with Mask R-CNN (OpenCV library)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explain step by step how to implement Instance Segmentation in video.   \n",
    "The goal is to generate a mask for each object in our video, such that we will be able to segment the foreground object from the background.   \n",
    "Instance segmentation algorithms compute a pixel-wise mask for every detected object in the frame image.   \n",
    "We will perform Instance Segmentation using Mask R-CNN architecture as instance segmentation algorithm. The Mask R-CNN algorithm is built upon the Faster R-CNN architecture.  Faster R-CNN is a popular object detection framework, and Mask R-CNN extends it through instance segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "First step, we need to import OpenCV and Numpy libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video\n",
    "\n",
    "Let's have a look at the video.\n",
    "\n",
    "Link (click on \"download\" and watch it):   [https://github.com/buropas/Image_and_Video_Segmentation/blob/main/test.mp4](https://github.com/buropas/Image_and_Video_Segmentation/blob/main/test.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"test.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look at the input video\n",
    "Video(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model Configuration and Pre-trained Weights\n",
    "We load:\n",
    "- the Mask R-CNN model weights (\"frozen_inference_graph.pb\"), which are pre-trained on the COCO dataset, and\n",
    "- the Mask R-CNN model configuration (\"mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASK R-CNN LOADED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "## Loading Mask R-CNN configuration file and pre-trained weights\n",
    "\n",
    "net = cv2.dnn.readNetFromTensorflow(\"frozen_inference_graph_coco.pb\",                   # weights path\n",
    "                                    \"mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\")     # config path\n",
    "                                    \n",
    "print(\"MASK R-CNN LOADED SUCCESSFULLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we need in order to load the model configuration and pre-trained weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes in the COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CLASSES: 90\n"
     ]
    }
   ],
   "source": [
    "# classes in COCO dataset\n",
    "classes = [\"person\",\"bicycle\",\"car\",\"motorcycle\",\"airplane\",\"bus\",\"train\",\"truck\",\"boat\",\"traffic light\",\n",
    "           \"fire hydrant\",\"street sign\",\"stop sign\",\"parking meter\",\"bench\",\"bird\",\"cat\",\"dog\",\"horse\",\n",
    "           \"sheep\",\"cow\",\"elephant\",\"bear\",\"zebra\",\"giraffe\",\"hat\",\"backpack\",\"umbrella\",\"shoe\",\"eye glasses\",\n",
    "           \"handbag\",\"tie\",\"suitcase\",\"frisbee\",\"skis\",\"snowboard\",\"sports ball\",\"kite\",\"baseball bat\",\n",
    "           \"baseball glove\",\"skateboard\",\"surfboard\",\"tennis racket\",\"bottle\",\"plate\",\"wine glass\",\n",
    "           \"cup\",\"fork\",\"knife\",\"spoon\",\"bowl\",\"banana\",\"apple\",\"sandwich\",\"orange\",\"broccoli\",\"carrot\",\n",
    "           \"hot dog\",\"pizza\",\"donut\",\"cake\",\"chair\",\"couch\",\"potted plant\",\"bed\",\"mirror\",\"dining table\",\n",
    "           \"window\",\"desk\",\"toilet\",\"door\",\"tv\",\"laptop\",\"mouse\",\"remote\",\"keyboard\",\"cell phone\",\"microwave\",\n",
    "           \"oven\",\"toaster\",\"sink\",\"refrigerator\",\"blender\",\"book\",\"clock\",\"vase\",\"scissors\",\"teddy bear\",\n",
    "           \"hair drier\",\"toothbrush\"]\n",
    "\n",
    "print(\"NUM CLASSES:\", len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing video \n",
    "Next step is to load the video and define the VideoWriter object in order to save our final video with object segmentation. The output video will be saved as \"segm_out_video.avi\".\n",
    "\n",
    "In the VideoWriter object we specify:\n",
    "- the output file name (segm_out_video.avi), \n",
    "- the FourCC code (a 4-byte code used to specify the video codec), \n",
    "- the number of frames per second (fps), \n",
    "- the frame size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading video \n",
    "filename = \"test.mp4\"                         # filename\n",
    "cap = cv2.VideoCapture(filename)              # loading video\n",
    "\n",
    "# We get the resolution of our video (width and height) and we convert from float to integer\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# We create VideoWriter object and define the codec. The output is stored in 'segm_out_video.avi' file.\n",
    "out_video = cv2.VideoWriter(\"segm_out_video.avi\",                        # output name\n",
    "                            cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),  # 4-byte code used to specify the video codec\n",
    "                                                                         # (we pass MJPG)\n",
    "                            10,                                          # number of frames per second (fps) \n",
    "                            (frame_width, frame_height)                  # frame size\n",
    "                            )\n",
    "\n",
    "# set font and color of text (to show class and confidence score)\n",
    "font = cv2.FONT_HERSHEY_PLAIN     # font\n",
    "text_color = (0,255,0)            # green color\n",
    "\n",
    "# random colors to distinguish between different classes (90 classes, 3 channels)\n",
    "colors = np.random.randint(0, 255, (90, 3))   # generate 90 random colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Object Segmentation\n",
    "\n",
    "\n",
    "The goal is to perform Video Object Segmentation: we want to automatically segment and generate pixel-wise masks for every detected object in our video.   \n",
    "Video Object Segmentation is a binary labeling problem aiming to separate foreground object(s) from the background region of a video.   \n",
    "So, the idea is that we process the video frame by frame.   \n",
    "For each frame:\n",
    "- we preprocess the frame and pass it as input into the network, then we run a forward pass to generate the network output,\n",
    "- as output of the network, we obtain detected objects with bounding boxes coordinates, confidence scores, classes and predicted masks,\n",
    "- we filter out objects detected with a confidence score lower than a specific threshold.\n",
    "- Then, For the remaining detected objects, we extract the bounding boxes and the associated mask.    \n",
    "  The predicted mask is only 15 x 15 pixels, so we need to resize the mask back in order to adapt the mask to the size   \n",
    "  of the object in the original image.\n",
    "- Finally, we find contours of the masks and we fill the area of the detected objects using random colors. Each class has its own color. We also create an overlay image in order to obtain transparency in the area of the detected \n",
    "  objects.\n",
    "\n",
    "At the end, we save the video with Object Segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():     # while the capture is correctly initialized...\n",
    "\n",
    "    # We process the video frame-by-frame\n",
    "    \n",
    "    ret, img = cap.read()           # we read each frame (img) from the video\n",
    "                                    # we also retrieve ret, which is a boolean value. \n",
    "                                    # ret is True if the frame is read correctly    \n",
    "    \n",
    "    if ret == True:    # if the frame is read correctly, go on...\n",
    "        \n",
    "        # make 2 copies of the frame image (the first will be the final output, while the second will be the \n",
    "        # overlay on top)\n",
    "        output = img.copy()             # copy of the original frame \n",
    "        overlay = img.copy()            # copy of the original frame\n",
    "\n",
    "        height, width, _ = img.shape    # retrieve shape from image (frame)\n",
    "        \n",
    "\n",
    "        ## IMAGE PREPROCESSING\n",
    "        ## Using blob function of opencv to preprocess frame (image) \n",
    "        # (The cv2.dnn.blobFromImage function returns a blob which is our input image with color swapping)\n",
    "        blob = cv2.dnn.blobFromImage(img, swapRB=True)  \n",
    "        \n",
    "        ## NETWORK PREDICTIONS (Output)\n",
    "        net.setInput(blob)                                                     # set blob as input to the network\n",
    "        boxes, masks = net.forward([\"detection_out_final\", \"detection_masks\"]) # runs a forward pass... \n",
    "                                                                               # ...to compute the net output \n",
    "\n",
    "        num_detections = boxes.shape[2]        # number of detected objects in the frame\n",
    "\n",
    "        for i in range(num_detections):        # for each of the detected objects...\n",
    "\n",
    "            box = boxes[0,0,i]                 # single detected object\n",
    "            class_id = int(box[1])             # the class associated with the detected object is the second element\n",
    "            confidence_score = box[2]          # the confidence score for the detected object is the third element\n",
    "\n",
    "            if confidence_score > 0.5:       # if the confidence score of the detected object is above a \n",
    "                                             # specific threshold, we keep on extracting box coordinates and \n",
    "                                             # mask associated with that object\n",
    "\n",
    "                label = str(classes[class_id])    # class associated with the object\n",
    "\n",
    "                x1, y1, x2, y2 = box[3:]          # box coordinates  (last 4 elements)\n",
    "\n",
    "                # we multiply the coordinates for the width and height of our original image\n",
    "                x1 = int(x1 * width)\n",
    "                y1 = int(y1 * height)\n",
    "                x2 = int(x2 * width)\n",
    "                y2 = int(y2 * height)\n",
    "\n",
    "                object_area = overlay[y1:y2, x1:x2]    # area of the detected object\n",
    "\n",
    "                object_height, object_width, _ = object_area.shape   # height and width of the detected object\n",
    "\n",
    "\n",
    "                ## MASK ##\n",
    "                ##########\n",
    "\n",
    "                ## We extract the pixel-wise segmentation (mask) for the detected object, \n",
    "                ## we resize the mask such that it's the same dimensions of the bounding box of the detected object\n",
    "                ## finally, we threshold to create a binary mask.\n",
    "\n",
    "                mask = masks[i, class_id]     # mask associated with the detected object and its predicted class id\n",
    "\n",
    "                # The predicted mask is only 15 x 15 pixels so we resize the mask back to the original input object \n",
    "                # dimensions. We need to adapt the mask to the size of the object in the original image\n",
    "                mask = cv2.resize(mask, (object_width, object_height))\n",
    "\n",
    "                # For every pixel in the mask, if the pixel value is smaller than the threshold, it is set to 0, \n",
    "                # otherwise it is set to a maximum value (255, white pixel). \n",
    "                # The function cv2.threshold is used to apply the thresholding and we set a binary thresholding. \n",
    "                _, mask = cv2.threshold(mask, 0.5, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "                mask = np.array(mask, np.uint8)     # convert to array of integer \n",
    "\n",
    "                # We find the countours of the mask (mask coordinates) \n",
    "                # Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "                # We fill the detected object with a random color for each detected object.\n",
    "                # Each class has its own color.\n",
    "                # fillPoly() fills an area bounded by several polygonal contours.\n",
    "                for cnt in contours:     # for each contour...\n",
    "                    cv2.fillPoly(object_area,            # area to fill\n",
    "                                 [cnt],                  # contours bounding the area\n",
    "                                 (int(colors[class_id][0]), int(colors[class_id][1]), int(colors[class_id][2]))# color\n",
    "                                 )\n",
    "                \n",
    "                # We put text (class and confidence score) on top of each detected object\n",
    "                cv2.putText(output, label + \" \" + str(round(confidence_score,2)), (x1, y1), font, 1.2, \n",
    "                            text_color, 2)   # text of the box \n",
    "\n",
    "\n",
    "        # Now, we apply the overlay.\n",
    "        # Overlay is the image that we want to “overlay” on top of the original image using a supplied level \n",
    "        # of alpha transparency.\n",
    "        alpha = 0.6\n",
    "        cv2.addWeighted(overlay,  # image that we want to “overlay” on top of the original image\n",
    "                        alpha,    # alpha transparency of the overlay (the closer to 0 the more transparent the \n",
    "                                  # overlay will appear)\n",
    "                        output,   # original source image\n",
    "                        1-alpha,  # beta parameter (1-alpha)\n",
    "                        0,        # gamma value — a scalar added to the weighted sum (we set it to 0)\n",
    "                        output    # our final output image\n",
    "                        )\n",
    "            \n",
    "        cv2.imshow(\"out\", output)      # display the current frame with detected objects and masks \n",
    "\n",
    "        out_video.write(output)        # the frame is saved for the final video\n",
    "\n",
    "        key = cv2.waitKey(1)        # wait 1 millisecond between each frame\n",
    "        if key == 27:               # if exit button, break and close\n",
    "            break\n",
    "    \n",
    "    \n",
    "    else:   # if the frame is not read correctly, break...\n",
    "        break\n",
    "    \n",
    "# Release everything when job is finished\n",
    "cap.release()\n",
    "out_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look at the final result..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video with Object Segmentation\n",
    "\n",
    "Link to Output Video (click on \"download\" and just watch or download it):   \n",
    "\n",
    "[https://github.com/buropas/Image_and_Video_Segmentation/blob/main/segm_out_video.avi](https://github.com/buropas/Image_and_Video_Segmentation/blob/main/segm_out_video.avi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
